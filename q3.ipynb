{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset and createing train,val,test matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = pathlib.Path(\"./ESC-50-master\")\n",
    "meta_data = pd.read_csv(base_path / \"meta/esc50.csv\")\n",
    "audio_folder = base_path / \"audio\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path_list=[]\n",
    "val_file_path_list=[]\n",
    "test_file_path_list=[]\n",
    "y_train=[]\n",
    "y_val=[]\n",
    "y_test=[]\n",
    "for file in audio_folder.iterdir():\n",
    "    if file.name in meta_data[meta_data['esc10'] == True]['filename'].to_list():\n",
    "        if file.name in meta_data[meta_data['fold'].isin([1,2,3]) ]['filename'].to_list():\n",
    "            train_file_path_list.append(file)\n",
    "            y_train.append(meta_data[meta_data['filename'] == file.name]['category'].values[0])\n",
    "        elif file.name in meta_data[meta_data['fold'].isin([4]) ]['filename'].to_list():\n",
    "            val_file_path_list.append(file)\n",
    "            y_val.append(meta_data[meta_data['filename'] == file.name]['category'].values[0])\n",
    "        elif file.name in meta_data[meta_data['fold'].isin([5]) ]['filename'].to_list():\n",
    "            test_file_path_list.append(file)\n",
    "            y_test.append(meta_data[meta_data['filename'] == file.name]['category'].values[0])\n",
    "print(\"Train files: \", len(train_file_path_list))\n",
    "print(\"Validation files: \", len(val_file_path_list))\n",
    "print(\"Test files: \", len(test_file_path_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel_spectrogram(audio_path, n_mels=128, win_ms=25, hop_ms=10, duration=5, sr=44100):\n",
    "    \n",
    "    \n",
    "    y, sr = librosa.load(audio_path, sr=None, duration=duration)\n",
    "    win_length = int(win_ms * sr / 1000)\n",
    "    hop_length = int(hop_ms * sr / 1000)\n",
    "    \n",
    "    mel_spectrogram = librosa.feature.melspectrogram(\n",
    "        y=y,\n",
    "        sr=sr,\n",
    "        n_mels=n_mels,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "    )\n",
    "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    \n",
    "    return mel_spectrogram_db[:,:500] # Dropping the last columns as advised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each array is of the shape no of sample ,128, 500\n",
    "X_train = np.stack([extract_mel_spectrogram(file) for file in train_file_path_list], axis=0)\n",
    "X_val = np.stack([extract_mel_spectrogram(file) for file in val_file_path_list], axis=0)\n",
    "X_test = np.stack([extract_mel_spectrogram(file) for file in test_file_path_list], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "LabelEncoder = LabelEncoder()\n",
    "y_train = LabelEncoder.fit_transform(y_train)\n",
    "y_val = LabelEncoder.transform(y_val)\n",
    "y_test = LabelEncoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part a A batch size of 32 is used throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train,).unsqueeze(1)\n",
    "X_val = torch.tensor(X_val).unsqueeze(1)\n",
    "X_test = torch.tensor(X_test).unsqueeze(1)\n",
    "y_train ,y_val, y_test = torch.tensor(y_train), torch.tensor(y_val), torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating datasets and dataloaders\n",
    " #TODO Mention in report I have used Relu activation function except for output layer I have used 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model and Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vanilla_cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(3, 3), stride=(1, 1))\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(3, 3), stride=(1, 1))\n",
    "        self.l1 = nn.LazyLinear(out_features=128)\n",
    "        self.l2 = nn.Linear(in_features=128, out_features=10)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        '''\n",
    "        X is 16,1,128,500  batch,num_channels, height, width \n",
    "        '''\n",
    "        h1 = self.conv1(X) # 16,16,126,498\n",
    "        h1_pool = self.pool(h1) # 16,16,124,496\n",
    "        z1 = torch.relu(h1_pool)  \n",
    "        h2 = self.conv2(z1) # 16,16,122,494\n",
    "        h2_pool = self.pool(h2) # 16,16,120,492\n",
    "        z2 = torch.relu(h2_pool)\n",
    "        z2_flat = z2.reshape(z2.shape[0], -1)\n",
    "        h3 = self.l1(z2_flat)\n",
    "        z3 = torch.relu(h3)\n",
    "        h4 = self.l2(z3) # 16,10\n",
    "        return h4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self,model,train_dl,val_dl,test_dl,epochs,optimizer,device):\n",
    "        self.model = model\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        self.test_dl = test_dl\n",
    "        self.epochs = epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.model.to(self.device)\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "    \n",
    "    def evaluate_loss(self,dl):\n",
    "        self.model.eval()\n",
    "        loss = 0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in dl:\n",
    "                xb = xb.to(self.device)\n",
    "                yb = yb.to(self.device)\n",
    "                pred = self.model(xb)\n",
    "                loss = loss+ self.loss_fn(pred, yb).item()*len(yb)\n",
    "                total_samples += len(yb)\n",
    "        return loss/total_samples\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        self.train_loss.append(self.evaluate_loss(self.train_dl))\n",
    "        self.val_loss.append(self.evaluate_loss(self.val_dl))\n",
    "        tqdm.write(f\"Initial Train Loss: {self.train_loss[-1]}, Validation Loss: {self.val_loss[-1]}\")\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            self.model.train()\n",
    "            for xb,yb in self.train_dl:\n",
    "                xb = xb.to(self.device)\n",
    "                yb = yb.to(self.device)\n",
    "                pred = self.model(xb)\n",
    "                loss = self.loss_fn(pred, yb)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            self.train_loss.append(self.evaluate_loss(self.train_dl))\n",
    "            self.val_loss.append(self.evaluate_loss(self.val_dl))    \n",
    "    def plot_loss(self,title):\n",
    "        plt.plot(self.train_loss, label='Training Loss',marker='*')\n",
    "        plt.plot(self.val_loss, label='Validation Loss',marker='o')\n",
    "        plt.xlabel('Number of Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.savefig(title + \".png\")\n",
    "        plt.show()\n",
    "    \n",
    "    def evaluate_accuracy(self,dl):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in dl:\n",
    "                xb = xb.to(self.device)\n",
    "                yb = yb.to(self.device)\n",
    "                pred = self.model(xb)\n",
    "                y_pred = torch.argmax(pred, dim=1)\n",
    "                correct += (y_pred == yb).sum().item()\n",
    "                total_samples += len(yb)\n",
    "        return correct/total_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the expertiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model_sgd = vanilla_cnn()\n",
    "optimizer = optim.SGD(model_sgd.parameters())\n",
    "trainer = Trainer(model_sgd,train_dl,val_dl,test_dl,epochs=num_epochs,optimizer=optimizer,device=device)\n",
    "trainer.train()\n",
    "trainer.plot_loss(\"SGD Loss\")\n",
    "train_accuracy = trainer.evaluate_accuracy(train_dl)\n",
    "val_accuracy = trainer.evaluate_accuracy(val_dl)\n",
    "test_accuracy = trainer.evaluate_accuracy(test_dl)\n",
    "print(f\"Train Accuracy with SGD: {train_accuracy}\")\n",
    "print(f\"Validation Accuracy with SGD: {val_accuracy}\")\n",
    "print(f\"Test Accuracy with SGD: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(trainer)\n",
    "del(model_sgd)\n",
    "del(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model_sgd_momentum = vanilla_cnn()\n",
    "optimizer = optim.SGD(model_sgd_momentum.parameters(), momentum=0.9)\n",
    "trainer = Trainer(model_sgd_momentum,train_dl,val_dl,test_dl,epochs=num_epochs,optimizer=optimizer,device=device)\n",
    "trainer.train()\n",
    "trainer.plot_loss(\"SGD with momentum\")\n",
    "train_accuracy = trainer.evaluate_accuracy(train_dl)\n",
    "val_accuracy = trainer.evaluate_accuracy(val_dl)\n",
    "test_accuracy = trainer.evaluate_accuracy(test_dl)\n",
    "print(f\"Train Accuracy with SGD with momentum: {train_accuracy}\")\n",
    "print(f\"Validation Accuracy with SGD with momentum: {val_accuracy}\")\n",
    "print(f\"Test Accuracy with SGD with momentum: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(trainer)\n",
    "del(model_sgd_momentum)\n",
    "del(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model_adam = vanilla_cnn()\n",
    "optimizer = optim.Adam(model_adam.parameters())\n",
    "trainer = Trainer(model_adam,train_dl,val_dl,test_dl,epochs=num_epochs,optimizer=optimizer,device=device)\n",
    "trainer.train()\n",
    "trainer.plot_loss(\"Adam Loss\")\n",
    "train_accuracy = trainer.evaluate_accuracy(train_dl)\n",
    "val_accuracy = trainer.evaluate_accuracy(val_dl)\n",
    "test_accuracy = trainer.evaluate_accuracy(test_dl)\n",
    "print(f\"Train Accuracy with Adam: {train_accuracy}\")\n",
    "print(f\"Validation Accuracy with Adam: {val_accuracy}\")\n",
    "print(f\"Test Accuracy with Adam: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(trainer)\n",
    "del(model_adam)\n",
    "del(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class normed_cnn(nn.Module):\n",
    "    def __init__(self,norm_type=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(3, 3), stride=(1, 1))\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(3, 3), stride=(1, 1))\n",
    "        self.l1 = nn.LazyLinear(out_features=128)\n",
    "        self.l2 = nn.Linear(in_features=128, out_features=10)\n",
    "        self.norm_type = norm_type\n",
    "        if norm_type == 'batch':\n",
    "            self.norm_layer = nn.BatchNorm2d(16)\n",
    "        elif norm_type == 'layer':\n",
    "            self.norm_layer = nn.LayerNorm([120, 492])    \n",
    "    def forward(self,X):\n",
    "        '''\n",
    "        X is 16,1,128,500  batch,num_channels, height, width \n",
    "        '''\n",
    "        h1 = self.conv1(X) # 16,16,126,498\n",
    "        h1_pool = self.pool(h1) # 16,16,124,496\n",
    "        z1 = torch.relu(h1_pool)  \n",
    "        h2 = self.conv2(z1) # 16,16,122,494\n",
    "        h2_pool = self.pool(h2) # 16,16,120,492\n",
    "        if self.norm_type is not None:\n",
    "            h2_pool = self.norm_layer(h2_pool)\n",
    "        z2 = torch.relu(h2_pool)\n",
    "        z2_flat = z2.reshape(z2.shape[0], -1)\n",
    "        h3 = self.l1(z2_flat)\n",
    "        z3 = torch.relu(h3)\n",
    "        h4 = self.l2(z3) # 16,10\n",
    "        return h4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the experiments for part b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "nonorm_model = normed_cnn(norm_type=None)\n",
    "optimizer = optim.Adam(nonorm_model.parameters())\n",
    "trainer = Trainer(nonorm_model,train_dl,val_dl,test_dl,epochs=num_epochs,optimizer=optimizer,device=device)\n",
    "trainer.train()\n",
    "trainer.plot_loss(\"No Norm Loss\")\n",
    "train_accuracy = trainer.evaluate_accuracy(train_dl)\n",
    "val_accuracy = trainer.evaluate_accuracy(val_dl)\n",
    "test_accuracy = trainer.evaluate_accuracy(test_dl)\n",
    "print(f\"Train Accuracy with No Norm: {train_accuracy}\")\n",
    "print(f\"Validation Accuracy with No Norm: {val_accuracy}\")\n",
    "print(f\"Test Accuracy with No Norm: {test_accuracy}\")\n",
    "del(trainer)\n",
    "del(nonorm_model)\n",
    "del(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "layernorm_model = normed_cnn(norm_type='layer')\n",
    "optimizer = optim.Adam(layernorm_model.parameters())\n",
    "trainer = Trainer(layernorm_model,train_dl,val_dl,test_dl,epochs=num_epochs,optimizer=optimizer,device=device)\n",
    "trainer.train()\n",
    "trainer.plot_loss(\"Layer Norm Loss\")\n",
    "train_accuracy = trainer.evaluate_accuracy(train_dl)\n",
    "val_accuracy = trainer.evaluate_accuracy(val_dl)\n",
    "test_accuracy = trainer.evaluate_accuracy(test_dl)\n",
    "print(f\"Train Accuracy with Layer Norm: {train_accuracy}\")\n",
    "print(f\"Validation Accuracy with Layer Norm: {val_accuracy}\")\n",
    "print(f\"Test Accuracy with Layer Norm: {test_accuracy}\")\n",
    "del(trainer)\n",
    "del(layernorm_model)\n",
    "del(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "batchnorm_model = normed_cnn(norm_type='batch')\n",
    "optimizer = optim.Adam(batchnorm_model.parameters())\n",
    "trainer = Trainer(batchnorm_model,train_dl,val_dl,test_dl,epochs=num_epochs,optimizer=optimizer,device=device)\n",
    "trainer.train()\n",
    "trainer.plot_loss(\"Batch Norm Loss\")\n",
    "train_accuracy = trainer.evaluate_accuracy(train_dl)\n",
    "val_accuracy = trainer.evaluate_accuracy(val_dl)\n",
    "test_accuracy = trainer.evaluate_accuracy(test_dl)\n",
    "print(f\"Train Accuracy with Batch Norm: {train_accuracy}\")\n",
    "print(f\"Validation Accuracy with Batch Norm: {val_accuracy}\")\n",
    "print(f\"Test Accuracy with Batch Norm: {test_accuracy}\")\n",
    "del(trainer)\n",
    "del(batchnorm_model)\n",
    "del(optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
