{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset and replacing positive with 1 and negative with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = pathlib.Path('IMDB_Dataset/IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['sentiment']=data_df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train test val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the data\n",
    "df = data_df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "df_train,df_val,df_test = df[:40000],df[40000:45000],df[45000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plot of word-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load glove embeddings\n",
    "import gensim.downloader as api\n",
    "glove_vectors = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To deal with cluttering due to a lot of words in the ten sentences I have used only two sentences from the validation set instead of 10\n",
    "#TODO : Change it to 10 sentences if no reply is received\n",
    "val_words = dict()\n",
    "for sentence in df_val['review'][:2]:\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in glove_vectors:\n",
    "            val_words[word.lower()] = glove_vectors[word.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_obj = PCA(n_components=2)\n",
    "pca_obj.fit(list(val_words.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the embeddings via pca and storing them in a dictionary\n",
    "val_pca_words = dict()\n",
    "for key,embedding in val_words.items():\n",
    "    val_pca_words[key] = pca_obj.transform([embedding])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracting x and y coordinates and corresponding words\n",
    "x_coords = [val_pca_words[word][0] for word in val_pca_words]\n",
    "y_coords = [val_pca_words[word][1] for word in val_pca_words]\n",
    "words = list(val_pca_words.keys())\n",
    "\n",
    "# Creating the scatter plot\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.scatter(x_coords, y_coords, alpha=0.5)\n",
    "\n",
    "# Annotating the words\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (x_coords[i], y_coords[i]), fontsize=14, alpha=0.7)\n",
    "\n",
    "plt.title(\"Word Embeddings Scatter Plot\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# TODO: Write the semantic clustering in the answer\n",
    "# The articles are clustered in the left portion of the plot along with common helping-verbs like was, is, are, etc.\n",
    "# The adjective words are clustered in the right portion of the plot. The words like \"wonderful\", \"innocent\", \"forget\" are clustered together.\n",
    "# Pronouns I and you are clustered together. at the bottom left \n",
    "# The words like \"movie\", \"film\", \"story\" are clustered together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating features from word to vec for the second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that creates a sentence representation words not in glove are dropped.\n",
    "def create_sentence_representation(sentence,glove_vectors):\n",
    "    sentence_representation = []\n",
    "    sentence = sentence.lower()\n",
    "    for word in sentence.split():\n",
    "        if word in glove_vectors:\n",
    "            sentence_representation.append(glove_vectors[word])\n",
    "    return sentence_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [torch.tensor(np.array(create_sentence_representation)(sentence,glove_vectors)) for sentence in df_train['review']]\n",
    "X_val = [torch.tensor(np.array(create_sentence_representation)(sentence,glove_vectors)) for sentence in df_val['review']]\n",
    "X_test = [torch.tensor(np.array(create_sentence_representation)(sentence,glove_vectors)) for sentence in df_test['review']]\n",
    "y_train = df_train['sentiment'].values\n",
    "y_val = df_val['sentiment'].values\n",
    "y_test = df_test['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO : Mention the initial Learning rate in the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def my_collate(batch):\n",
    "    len_list = []\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    for x,y in batch:\n",
    "        len_list.append(x.shape[0])\n",
    "        x_list.append(x)\n",
    "        y_list.append(y)\n",
    "    x_list = pad_sequence(x_list,batch_first=True) # batch,max_seq_len,100\n",
    "    y_list = torch.tensor(y_list)\n",
    "    len_list = torch.tensor(len_list)\n",
    "    return x_list,y_list.float(),len_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloder(X,y,batch_size):\n",
    "    dataset = list(zip(X,y))\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size,shuffle=True,collate_fn=my_collate)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = create_dataloder(X_train,y_train,batch_size)\n",
    "val_dl = create_dataloder(X_val,y_val,batch_size)\n",
    "test_dl = create_dataloder(X_test,y_test,batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Training LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_model_avg_pool(nn.Module):\n",
    "    def __init__(self,hidden_size=256,num_layers=2):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Linear(hidden_size,1)\n",
    "        self.lstm = nn.LSTM(100,hidden_size,num_layers,batch_first=True)\n",
    "    \n",
    "    def make_mask(self,len_list,device):\n",
    "        batch_size = len_list.shape[0]\n",
    "        max_len = len_list.max()\n",
    "        mask = len_list.reshape(batch_size,1) > torch.arange(max_len).reshape(1,max_len).to(device) # batch,max_len\n",
    "        return mask.reshape(batch_size,max_len,1)\n",
    "    def forward(self,x,len_list,device):\n",
    "        '''\n",
    "        len_list : batch to be used to make a mask\n",
    "        '''\n",
    "        out,_ = self.lstm(x) #  batch,seq_len,hidde_size\n",
    "        mask = self.make_mask(len_list,device)\n",
    "        out = out*mask # batch,seq_len,hidden_size\n",
    "        out_pool = out.sum(dim=1)/len_list.reshape(-1,1) # batch,hidden_size\n",
    "        return self.classifier(out_pool) # batch,1\n",
    "    def predict(self,x,len_list):\n",
    "        res = self.forward(x,len_list)\n",
    "        return torch.where(res>0,1,0) # batch,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An abstract class that will be used to train the models inspired from the assignments of NLP course\n",
    "class ModelTrainer:\n",
    "    def __init__(self,model,train_dl,val_dl,optimizer=None,batch_size=32,epochs=25):\n",
    "        self.model = model\n",
    "        if optimizer is None:\n",
    "            self.optimizer = optim.Adam(model.parameters(),lr=1e-3)\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.epochs = epochs\n",
    "        self.train_loss = {}\n",
    "        self.val_loss = {}\n",
    "        self.batch_size = batch_size\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        self.steps = 0\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "    def train(self):\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "                for xb,yb,len_list in train_dl:\n",
    "                    self.model.train()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    xb,yb,len_list = xb.to(self.device),yb.to(self.device),len_list.to(self.device)\n",
    "                    y_pred = self.model(xb,len_list,self.device)\n",
    "                    loss = self.criterion(y_pred,yb.unsqueeze(1))\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    self.train_loss[self.steps] = loss.item()\n",
    "                    if self.steps%100 == 0:\n",
    "                        val_loss = self.get_validation_loss(X_val,y_val)\n",
    "                        self.val_loss[self.steps] = val_loss\n",
    "                        tqdm.write(f\"Epoch : {epoch} Step : {self.steps} Train Loss : {loss.item()} Validation Loss : {val_loss}\")\n",
    "                    self.steps += 1\n",
    "\n",
    "                    \n",
    "                \n",
    "    def get_validation_loss(self,X_val,y_val):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for xb,yb,len_list in val_dl:\n",
    "                xb,yb,len_list = xb.to(self.device),yb.to(self.device),len_list.to(self.device)\n",
    "                y_pred = self.model(xb,len_list,self.device)\n",
    "                loss = self.criterion(y_pred,yb.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "            return val_loss/len(val_dl)\n",
    "        \n",
    "    def plot_losses(self,title_str):\n",
    "        plt.plot(list(self.train_loss.keys()),list(self.train_loss.values()),label='Train Loss')\n",
    "        plt.plot(list(self.val_loss.keys()),list(self.val_loss.values()),label='Validation Loss')\n",
    "        plt.title(title_str)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_test_accuracy(self,test_dl):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for xb,yb,len_list in test_dl:\n",
    "                xb,yb,len_list = xb.to(self.device),yb.to(self.device),len_list.to(self.device)\n",
    "                y_pred = self.model.predict(xb,len_list,self.device)\n",
    "                correct += (y_pred == yb.unsqueeze(1)).sum().item()\n",
    "                total += yb.shape[0]\n",
    "            return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model_avg_pool_obj = lstm_model_avg_pool()\n",
    "model_trainer = ModelTrainer(lstm_model_avg_pool_obj,train_dl=train_dl,val_dl=val_dl)\n",
    "model_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Accuracy lstm avg pool: {model_trainer.get_test_accuracy(test_dl)}\")\n",
    "model_trainer.plot_losses('Train and Validation Losses LSTM Avg Pool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training a attention based pooling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_model_attention(nn.Module):\n",
    "    def __init__(self,hidden_size=256,num_layers=2):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Linear(hidden_size,1)\n",
    "        self.lstm = nn.LSTM(100,hidden_size,num_layers,batch_first=True)\n",
    "        self.attention = nn.Linear(hidden_size,1)\n",
    "        \n",
    "    def forward(self,x,len_list,device):\n",
    "        out,_ = self.lstm(x) # batch,seq_len,hidde_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
