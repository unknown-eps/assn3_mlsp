{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset and replacing positive with 1 and negative with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = pathlib.Path('IMDB_Dataset/IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['sentiment']=data_df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train test val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the data\n",
    "df = data_df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "df_train,df_val,df_test = df[:40000],df[40000:45000],df[45000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plot of word-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load glove embeddings\n",
    "import gensim.downloader as api\n",
    "glove_vectors = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To deal with cluttering due to a lot of words in the ten sentences I have used only two sentences from the validation set instead of 10\n",
    "#TODO : Change it to 10 sentences if no reply is received\n",
    "val_words = dict()\n",
    "for sentence in df_val['review'][:2]:\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in glove_vectors:\n",
    "            val_words[word.lower()] = glove_vectors[word.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_obj = PCA(n_components=2)\n",
    "pca_obj.fit(list(val_words.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the embeddings via pca and storing them in a dictionary\n",
    "val_pca_words = dict()\n",
    "for key,embedding in val_words.items():\n",
    "    val_pca_words[key] = pca_obj.transform([embedding])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracting x and y coordinates and corresponding words\n",
    "x_coords = [val_pca_words[word][0] for word in val_pca_words]\n",
    "y_coords = [val_pca_words[word][1] for word in val_pca_words]\n",
    "words = list(val_pca_words.keys())\n",
    "\n",
    "# Creating the scatter plot\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.scatter(x_coords, y_coords, alpha=0.5)\n",
    "\n",
    "# Annotating the words\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (x_coords[i], y_coords[i]), fontsize=14, alpha=0.7)\n",
    "\n",
    "plt.title(\"Word Embeddings Scatter Plot\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# TODO: Write the semantic clustering in the answer\n",
    "# The articles are clustered in the left portion of the plot along with common helping-verbs like was, is, are, etc.\n",
    "# The adjective words are clustered in the right portion of the plot. The words like \"wonderful\", \"innocent\", \"forget\" are clustered together.\n",
    "# Pronouns I and you are clustered together. at the bottom left \n",
    "# The words like \"movie\", \"film\", \"story\" are clustered together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating features from word to vec for the second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that creates a sentence representation words not in glove are dropped.\n",
    "def create_sentence_representation(sentence,glove_vectors):\n",
    "    sentence_representation = []\n",
    "    sentence = sentence.lower()\n",
    "    for word in sentence.split():\n",
    "        if word in glove_vectors:\n",
    "            sentence_representation.append(glove_vectors[word])\n",
    "    return sentence_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [torch.tensor(np.array(create_sentence_representation(sentence,glove_vectors))) for sentence in df_train['review']]\n",
    "X_val = [torch.tensor(np.array(create_sentence_representation(sentence,glove_vectors))) for sentence in df_val['review']]\n",
    "X_test = [torch.tensor(np.array(create_sentence_representation(sentence,glove_vectors))) for sentence in df_test['review']]\n",
    "y_train = df_train['sentiment'].values\n",
    "y_val = df_val['sentiment'].values\n",
    "y_test = df_test['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO : Mention the initial Learning rate in the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def my_collate(batch):\n",
    "    len_list = []\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    for x,y in batch:\n",
    "        len_list.append(x.shape[0])\n",
    "        x_list.append(x)\n",
    "        y_list.append(y)\n",
    "    x_list = pad_sequence(x_list,batch_first=True) # batch,max_seq_len,100\n",
    "    y_list = torch.tensor(y_list)\n",
    "    len_list = torch.tensor(len_list)\n",
    "    return x_list,y_list.float(),len_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloder(X,y,batch_size):\n",
    "    dataset = list(zip(X,y))\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size,shuffle=True,collate_fn=my_collate)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = create_dataloder(X_train,y_train,batch_size)\n",
    "val_dl = create_dataloder(X_val,y_val,batch_size)\n",
    "test_dl = create_dataloder(X_test,y_test,batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Training LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_model_avg_pool(nn.Module):\n",
    "    def __init__(self,hidden_size=256,num_layers=2):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Linear(hidden_size,1)\n",
    "        self.lstm = nn.LSTM(100,hidden_size,num_layers,batch_first=True)\n",
    "    \n",
    "    def make_mask(self,len_list,device):\n",
    "        batch_size = len_list.shape[0]\n",
    "        max_len = len_list.max()\n",
    "        mask = len_list.reshape(batch_size,1) > torch.arange(max_len).reshape(1,max_len).to(device) # batch,max_len\n",
    "        return mask.reshape(batch_size,max_len,1)\n",
    "    def forward(self,x,len_list,device):\n",
    "        '''\n",
    "        len_list : batch to be used to make a mask\n",
    "        '''\n",
    "        out,_ = self.lstm(x) #  batch,seq_len,hidde_size\n",
    "        mask = self.make_mask(len_list,device)\n",
    "        out = out*mask # batch,seq_len,hidden_size\n",
    "        out_pool = out.sum(dim=1)/len_list.reshape(-1,1) # batch,hidden_size\n",
    "        return self.classifier(out_pool) # batch,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An abstract class that will be used to train the models inspired from the assignments of NLP course\n",
    "class ModelTrainer:\n",
    "    def __init__(self,model,train_dl,val_dl,optimizer=None,batch_size=32,epochs=25):\n",
    "        self.model = model\n",
    "        if optimizer is None:\n",
    "            self.optimizer = optim.Adam(model.parameters(),lr=1e-3)\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.epochs = epochs\n",
    "        self.train_loss = {}\n",
    "        self.val_loss = {}\n",
    "        self.batch_size = batch_size\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        self.steps = 0\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "    def train(self):\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "                for xb,yb,len_list in train_dl:\n",
    "                    self.model.train()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    xb,yb,len_list = xb.to(self.device),yb.to(self.device),len_list.to(self.device)\n",
    "                    y_pred = self.model(xb,len_list,self.device)\n",
    "                    loss = self.criterion(y_pred,yb.unsqueeze(1))\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    self.steps += 1\n",
    "                self.train_loss[epoch] = self.get_validation_loss(self.train_dl)\n",
    "                self.val_loss[epoch] = self.get_validation_loss(self.val_dl)\n",
    "                tqdm.write(f'Epoch : {epoch} Train Loss : {self.train_loss[epoch]} Validation Loss : {self.val_loss[epoch]}')                    \n",
    "                \n",
    "    def get_validation_loss(self,set_dl):\n",
    "        self.model.eval()\n",
    "        val_dl = set_dl\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            total_len = 0\n",
    "            for xb,yb,len_list in val_dl:\n",
    "                xb,yb,len_list = xb.to(self.device),yb.to(self.device),len_list.to(self.device)\n",
    "                y_pred = self.model(xb,len_list,self.device)\n",
    "                loss = self.criterion(y_pred,yb.unsqueeze(1))\n",
    "                val_loss += loss.item()*len(yb)\n",
    "                total_len += len(yb)\n",
    "            return val_loss/total_len\n",
    "        \n",
    "    def plot_losses(self,title_str):\n",
    "        plt.plot(list(self.train_loss.keys()),list(self.train_loss.values()),label='Train Loss',marker='*')\n",
    "        plt.plot(list(self.val_loss.keys()),list(self.val_loss.values()),label='Validation Loss',marker='o')\n",
    "        plt.title(title_str)\n",
    "        plt.xlabel('Number of Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(title_str+'.png')\n",
    "        plt.show()\n",
    "    \n",
    "    def get_test_accuracy(self,test_dl):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for xb,yb,len_list in test_dl:\n",
    "                xb,yb,len_list = xb.to(self.device),yb.to(self.device),len_list.to(self.device)\n",
    "                y_pred = self.model.forward(xb,len_list,self.device)\n",
    "                y_pred = torch.where(y_pred>0,1,0)\n",
    "                correct += (y_pred == yb.unsqueeze(1)).sum().item()\n",
    "                total += yb.shape[0]\n",
    "            return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model_avg_pool_obj = lstm_model_avg_pool()\n",
    "model_trainer = ModelTrainer(lstm_model_avg_pool_obj,train_dl=train_dl,val_dl=val_dl)\n",
    "model_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Accuracy lstm avg pool: {model_trainer.get_test_accuracy(test_dl)}\")\n",
    "model_trainer.plot_losses('Train and Validation Losses LSTM Avg Pool')\n",
    "del(lstm_model_avg_pool_obj)\n",
    "del(model_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training a attention based pooling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_model_attention(nn.Module):\n",
    "    def __init__(self,hidden_size=256,num_layers=2):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Linear(hidden_size,1)\n",
    "        self.lstm = nn.LSTM(100,hidden_size,num_layers,batch_first=True)\n",
    "        self.V = nn.Linear(hidden_size,1)\n",
    "        self.W = nn.Linear(hidden_size,hidden_size)\n",
    "    def attention(self,out):\n",
    "        '''\n",
    "        out : batch,max_seq_len,hidden_size\n",
    "        Implements attention socres as v.Tanh(W*out+b)\n",
    "        '''\n",
    "        unnorm_attn_scores = self.V(torch.tanh(self.W(out)))\n",
    "        return unnorm_attn_scores # batch,max_seq_len,1\n",
    "    def make_mask(self,len_list,device):\n",
    "        batch_size = len_list.shape[0]\n",
    "        max_len = len_list.max()\n",
    "        mask = len_list.reshape(batch_size,1) > torch.arange(max_len).reshape(1,max_len).to(device) # batch,max_len\n",
    "        return mask.reshape(batch_size,max_len,1)\n",
    "            \n",
    "    def forward(self,x,len_list,device):\n",
    "        out,_ = self.lstm(x) # batch,max_seq_len,hidde_size\n",
    "        unnorm_scores = self.attention(out) # batch,max_seq_len,1\n",
    "        mask = self.make_mask(len_list,device)\n",
    "        attn_scores = torch.where(mask,unnorm_scores,-float('inf')) # batch,max_seq_len,1\n",
    "        attn_scores = torch.softmax(attn_scores,dim=1).reshape(-1,1,len_list.max()) # batch,1,max_seq_len\n",
    "        out_context = attn_scores@out # batch,1,hidden_size\n",
    "        final_out = self.classifier(out_context).reshape(-1,1) # batch,1\n",
    "        return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_model_attention_obj = lstm_model_attention()\n",
    "model_trainer = ModelTrainer(lst_model_attention_obj,train_dl=train_dl,val_dl=val_dl)\n",
    "model_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Accuracy lstm attention: {model_trainer.get_test_accuracy(test_dl)}\")\n",
    "model_trainer.plot_losses('Train and Validation Losses LSTM Attention')\n",
    "del(lst_model_attention_obj)\n",
    "del(model_trainer)\n",
    "#TODO: Write that tanh attention is used in the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing and training a transformer encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_encoder_model(nn.Module):\n",
    "    def __init__(self,hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Linear(hidden_size,1)\n",
    "        self.Q = nn.Linear(hidden_size,hidden_size,bias=False)\n",
    "        self.K = nn.Linear(hidden_size,hidden_size,bias=False)\n",
    "        self.V = nn.Linear(hidden_size,hidden_size,bias=False)\n",
    "    \n",
    "    def make_mask(self,len_list,device):\n",
    "        batch_size = len_list.shape[0]\n",
    "        max_len = len_list.max()\n",
    "        mask = len_list.reshape(batch_size,1) > torch.arange(max_len).reshape(1,max_len).to(device) # batch,max_len\n",
    "        return mask.reshape(batch_size,max_len,1)\n",
    "\n",
    "\n",
    "    def forward(self,x,len_list,device):\n",
    "        query= self.Q(x) # batch,max_seq_len,hidden_size\n",
    "        key = self.K(x) \n",
    "        vals = self.V(x)\n",
    "        mask = self.make_mask(len_list,device)\n",
    "        attn_scores = query@key.transpose(1,2) # batch,max_seq_len,max_seq_len\n",
    "        scaled_attn_scores = attn_scores/np.sqrt(query.shape[-1]) # batch,max_seq_len,max_seq_len\n",
    "        scaled_attn_scores = scaled_attn_scores.transpose(1,2) # batch,max_seq_len,max_seq_len\n",
    "        masked_attn_scores = torch.where(mask,scaled_attn_scores,-float('inf')).transpose(1,2) # batch,max_seq_len,max_seq_len\n",
    "        normalized_attn_scores = torch.softmax(masked_attn_scores,dim=2)\n",
    "        out = normalized_attn_scores@vals # batch,max_seq_len,hidden_size\n",
    "        out_masked = out*mask\n",
    "        out_masked_comb = out_masked.sum(dim=1)/len_list.reshape(-1,1)\n",
    "        return self.classifier(out_masked_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_encoder_model_obj = transformer_encoder_model()\n",
    "model_trainer = ModelTrainer(transformer_encoder_model_obj,train_dl=train_dl,val_dl=val_dl)\n",
    "model_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Accuracy transformer encoder: {model_trainer.get_test_accuracy(test_dl)}\")\n",
    "model_trainer.plot_losses('Train and Validation Losses Transformer Encoder')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
